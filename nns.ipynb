{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def windows(data, window_size):\n",
    "    start = 0\n",
    "    while start < len(data):\n",
    "        yield start, start + window_size\n",
    "        start += (window_size // 2)\n",
    "\n",
    "def extract_features(parent_dir, sub_dirs, file_ext=\"*.mp3\", bands = 60, frames = 41):\n",
    "    window_size = 512 * (frames - 1)\n",
    "    log_specgrams = []\n",
    "    labels = []\n",
    "    for l, sub_dir in enumerate(sub_dirs):\n",
    "        for fn in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)):\n",
    "            sound, sr = librosa.load(fn)\n",
    "            label = fn.split('/')[-1].split('-')[-2]\n",
    "            for (start, end) in windows(sound, window_size):\n",
    "                if(len(sound[start:end]) == window_size):\n",
    "                    signal = sound[start:end]\n",
    "                    melspec = librosa.feature.melspectrogram(signal, n_mels = bands)\n",
    "                    logspec = librosa.logamplitude(melspec)\n",
    "                    logspec = logspec.T.flatten()[:, np.newaxis].T\n",
    "                    log_specgrams.append(logspec)\n",
    "                    labels.append(label)\n",
    "            \n",
    "    log_specgrams = np.asarray(log_specgrams).reshape(len(log_specgrams), bands, frames, 1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis = 3)\n",
    "    for i in range(len(features)):\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "    \n",
    "    return np.array(features), np.array(labels, dtype = np.int)\n",
    "\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels, n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parent_dir = 'dataset/train'\n",
    "tr_sub_dirs = ['fold1', 'fold2', 'fold4', 'fold5', 'fold6', 'fold7', 'fold8', 'fold9']\n",
    "train_dataset, train_labels = extract_features(parent_dir, tr_sub_dirs)\n",
    "train_labels = one_hot_encode(train_labels)\n",
    "\n",
    "valid_sub_dirs = ['fold4']\n",
    "valid_dataset, valid_labels = extract_features(parent_dir, valid_sub_dirs)\n",
    "valid_labels = one_hot_encode(valid_labels)\n",
    "\n",
    "ts_sub_dirs = ['fold3']\n",
    "test_dataset, test_labels = extract_features(parent_dir, ts_sub_dirs)\n",
    "test_labels = one_hot_encode(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d5e68396f3f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_dataset.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_labels.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_dataset.pickle', 'wb') as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "\n",
    "with open('train_labels.pickle', 'wb') as f:\n",
    "    pickle.dump(train_labels, f)\n",
    "\n",
    "with open('valid_dataset.pickle', 'wb') as f:\n",
    "    pickle.dump(valid_dataset, f)\n",
    "\n",
    "with open('valid_labels.pickle', 'wb') as f:\n",
    "    pickle.dump(valid_labels, f)\n",
    "\n",
    "with open('test_dataset.pickle', 'wb') as f:\n",
    "    pickle.dump(test_dataset, f)\n",
    "\n",
    "with open('test_labels.pickle', 'wb') as f:\n",
    "    pickle.dump(test_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('train_dataset.pickle', 'rb') as f:\n",
    "#     train_dataset = pickle.load(f)\n",
    "\n",
    "# with open('train_lables.pickle', 'rb') as f:\n",
    "#     train_labels = pickle.load(f)\n",
    "\n",
    "with open('dataset_features/valid_dataset.pickle', 'rb') as f:\n",
    "    valid_dataset = pickle.load(f)\n",
    "\n",
    "with open('dataset_features/valid_labels.pickle', 'rb') as f:\n",
    "    valid_labels = pickle.load(f)\n",
    "\n",
    "with open('dataset_features/test_dataset.pickle', 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "\n",
    "with open('dataset_features/test_labels.pickle', 'rb') as f:\n",
    "    test_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8260, 60, 41, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, TimeDistributed, Bidirectional, Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, Dropout\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_ds = valid_dataset[:, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences = True, input_shape=(60, 41)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(128, return_sequences = False))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "1..2..3..4..5..6..7..8..9..10..11..12..13..14..15..16..17..18..19..20..21..22..23..24..25..26..27..28..29..30..31..32..33..34..35..36..37..38..39..40..41..42..43..44..45..46..47..48..49..50..51..52..53..54..55..56..57..58..59..60..61..62..63..64..65..66..67..68..69..70..71..72..73..74..75..76..77..78..79..80..81..82..83..84..85..86..87..88..89..90..91..92..93..94..95..96..97..98..99..fitted!\n",
      "Predicting...\n",
      "done!\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.37      0.40      0.39       938\n",
      "          1       0.48      0.44      0.46       892\n",
      "          2       0.19      0.13      0.15       644\n",
      "          3       0.15      0.09      0.11       770\n",
      "          4       0.30      0.17      0.21       811\n",
      "          5       0.19      0.45      0.26       755\n",
      "          6       0.10      0.06      0.08       878\n",
      "          7       0.28      0.20      0.23       796\n",
      "          8       0.21      0.24      0.22       948\n",
      "          9       0.21      0.27      0.23       828\n",
      "\n",
      "avg / total       0.25      0.25      0.24      8260\n",
      "\n",
      "0.2486682808716707\n"
     ]
    }
   ],
   "source": [
    "print('Fitting...')\n",
    "for iteration in range(1, 100):\n",
    "    sys.stdout.write('{}..'.format(iteration))\n",
    "    tr_ds_sh, tr_lb_sh = shuffle(tr_ds, valid_labels)\n",
    "    model.fit(tr_ds_sh, tr_lb_sh, batch_size=len(tr_ds), epochs=1, verbose=0)\n",
    "print('fitted!')\n",
    "print('Predicting...')\n",
    "preds = model.predict(test_dataset[:, :, :, 0], verbose=0)\n",
    "print('done!')\n",
    "preds = np.array(list(map(lambda x: list(map(lambda i: 1 if i==max(x) else 0, x)), preds)))\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(test_labels, preds))\n",
    "print(accuracy_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(128, (57, 6), input_shape=(60, 41, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(128, (1, 3)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7303 samples, validate on 812 samples\n",
      "Epoch 1/33\n",
      "7303/7303 [==============================] - 26s 4ms/step - loss: 14.2730 - acc: 0.1086 - val_loss: 15.3638 - val_acc: 0.0468\n",
      "Epoch 2/33\n",
      "7303/7303 [==============================] - 25s 3ms/step - loss: 14.1872 - acc: 0.1197 - val_loss: 14.8279 - val_acc: 0.0800\n",
      "Epoch 3/33\n",
      "7303/7303 [==============================] - 25s 3ms/step - loss: 14.2178 - acc: 0.1179 - val_loss: 14.8279 - val_acc: 0.0800\n",
      "Epoch 4/33\n",
      "7303/7303 [==============================] - 25s 3ms/step - loss: 14.2178 - acc: 0.1179 - val_loss: 14.8279 - val_acc: 0.0800\n",
      "Epoch 5/33\n",
      "7303/7303 [==============================] - 25s 3ms/step - loss: 14.2156 - acc: 0.1180 - val_loss: 14.8279 - val_acc: 0.0800\n",
      "Epoch 6/33\n",
      "7303/7303 [==============================] - 25s 3ms/step - loss: 14.2156 - acc: 0.1180 - val_loss: 14.8279 - val_acc: 0.0800\n",
      "Epoch 7/33\n",
      "7303/7303 [==============================] - 25s 3ms/step - loss: 14.2178 - acc: 0.1179 - val_loss: 14.8279 - val_acc: 0.0800\n",
      "Epoch 8/33\n",
      "7303/7303 [==============================] - 25s 3ms/step - loss: 14.2156 - acc: 0.1180 - val_loss: 14.8279 - val_acc: 0.0800\n",
      "Epoch 9/33\n",
      "1500/7303 [=====>........................] - ETA: 20s - loss: 14.3559 - acc: 0.1093"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b71076f185ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/maga/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/envs/maga/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/envs/maga/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/maga/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/maga/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/maga/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/maga/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/maga/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/maga/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(valid_dataset, valid_labels, epochs=33, batch_size=100,  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## почему могло не получиться\n",
    "\n",
    "- мел-кепстральные коэффициенты ничего не говорят\n",
    "- грязные данные\n",
    "- слишком большой learning rate\n",
    "- нельзя верить тому, как ты год назад делал датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# окей, а что ещё у нас есть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть большая куча англоязычных субтитров с отметками звуков. Может, можно как-то скластеризовать на условные жанры по звукам?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# распаковываем субтитры\n",
    "folder = '/Users/Basilis/Documents/maga/ML/diplom/'\n",
    "if not os.path.exists(os.path.join(folder, 'txtfiles')):\n",
    "    os.mkdir(os.path.join(folder, 'txtfiles'))\n",
    "for root, dirs, files in os.walk('/Users/Basilis/Downloads/files/'):\n",
    "    for fn in files:\n",
    "        if fn.endswith('.gz'):\n",
    "            if not fn.endswith('.txt.gz'):\n",
    "                with gzip.open(os.path.join(root, fn), 'rb') as f_in, open(os.path.join(folder, 'txtfiles', fn[:-3]), 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import *\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, make_scorer, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680\n",
      "1\r\n",
      "00:00:24,991 --> 00:00:27,494\r\n",
      "[ Door Buzzer Buzzing ]\r\n",
      "DADDY.\r\n",
      "\r\n",
      "2\r\n",
      "00:00:30,497 --> 00:00:32,499\r\n",
      "DADDY, ARE YOU HOME ?\r\n",
      "MAYBE HE ISN'T.\r\n",
      "\r\n",
      "3\r\n",
      "00:00:32,999 --> 00:00:35,001\r\n",
      "OF COURSE HE IS.\r\n",
      "DADDY !\r\n",
      "\r\n",
      "4\r\n",
      "00:00:35,001 --> 00:00:38,004\r\n",
      "[ Knocking ]\r\n",
      "FIND HIM ?\r\n",
      "\r\n",
      "5\r\n",
      "00:00:38,004 --> 00:00:42,6\n"
     ]
    }
   ],
   "source": [
    "# делаем выборку и засовываем в массив\n",
    "sample = random.sample(os.listdir('../txtfiles'), 2000)\n",
    "\n",
    "texts = []\n",
    "for fil in sample:\n",
    "    with open(os.path.join('../txtfiles', fil), 'rb') as f:\n",
    "        try:\n",
    "            text = f.read().decode('utf-8')\n",
    "            texts.append(text)\n",
    "        except:\n",
    "            continue\n",
    "print(len(texts))\n",
    "print(texts[300][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1\\r\\n00:00:02,240 --&gt; 00:00:04,470\\r\\nAnd it w...</td>\n",
       "      <td>it was at that moment that   realised th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1\\r\\n00:00:40,065 --&gt; 00:00:47,784\\r\\n(unearth...</td>\n",
       "      <td>(unearthly howling)       ?   is it?      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿1\\r\\n00:00:01,099 --&gt; 00:00:04,801\\r\\n&lt;i&gt;In 2...</td>\n",
       "      <td>﻿    i   an immortal tyrant named    /i      i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿1\\r\\n00:00:01,367 --&gt; 00:00:03,184\\r\\nOliver:...</td>\n",
       "      <td>﻿        name is    .        five years on a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1\\r\\n00:00:54,444 --&gt; 00:00:56,522\\r\\nHello, J...</td>\n",
       "      <td>.   .        taking this cat over to gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  1\\r\\n00:00:02,240 --> 00:00:04,470\\r\\nAnd it w...   \n",
       "1  1\\r\\n00:00:40,065 --> 00:00:47,784\\r\\n(unearth...   \n",
       "2  ﻿1\\r\\n00:00:01,099 --> 00:00:04,801\\r\\n<i>In 2...   \n",
       "3  ﻿1\\r\\n00:00:01,367 --> 00:00:03,184\\r\\nOliver:...   \n",
       "4  1\\r\\n00:00:54,444 --> 00:00:56,522\\r\\nHello, J...   \n",
       "\n",
       "                                               clear  \n",
       "0        it was at that moment that   realised th...  \n",
       "1      (unearthly howling)       ?   is it?      ...  \n",
       "2  ﻿    i   an immortal tyrant named    /i      i...  \n",
       "3  ﻿        name is    .        five years on a h...  \n",
       "4         .   .        taking this cat over to gr...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# чистим и засовываем в датафрейм\n",
    "texts = pd.DataFrame({'text': texts})\n",
    "texts['clear'] = texts['text'].map(lambda x: re.sub('((\\r)?\\n)|(</?[^>]+>)', ' ', re.sub('[0-9\\[\\]#\"\\':\\-<>,]+', '', x)))\n",
    "texts['clear'] = texts['clear'].map(lambda x: re.sub('[A-Z][a-z]+', ' ', x).lower())\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del texts['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwds = stopwords.words('english') + ['like', 'oh', 'know', 'right', 'get', 'let', 'go', 'well', \n",
    "                                        'come', 'gonna', 'yeah', 'good', 'yes', 'got', 'one', 'hey', 'think', 'want', \n",
    "                                       'us', 'man', 'men', 'okay', 'need', 'see', 'back', 'would', 'oh', 'going', \n",
    "                                       'take']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting...\n",
      "fitted\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words=stopwds)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "#     ('svd', TruncatedSVD(n_components=100, random_state=27)),\n",
    "    ('norm', Normalizer() ),\n",
    "    ('clust', KMeans(n_clusters=10, random_state=75))\n",
    "])\n",
    "print('fitting...')\n",
    "pipeline.fit(texts['clear'])\n",
    "print('fitted')\n",
    "\n",
    "\n",
    "# explained_variance = pipeline.named_steps['svd'].explained_variance_ratio_.sum()\n",
    "# print(\"Explained variance of the SVD step: {:.0f}%\".format(explained_variance * 100))\n",
    "\n",
    "clust_labels = pipeline.named_steps['clust'].labels_\n",
    "# labels = texts['event_id']\n",
    "\n",
    "# print(\"Homogeneity:\", homogeneity_score(labels, clust_labels))\n",
    "# print(\"Completeness:\", completeness_score(labels, clust_labels))\n",
    "# print(\"V-measure\",  v_measure_score(labels, clust_labels))\n",
    "# print(\"Adjusted Rand-Index:\",  adjusted_rand_score(labels, clust_labels))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0\n",
      " dont\n",
      " grunting\n",
      " grunts\n",
      " growling\n",
      " screaming\n",
      " gasps\n",
      " snarling\n",
      " youre\n",
      " panting\n",
      " sighs\n",
      "Cluster 1\n",
      " music\n",
      " dont\n",
      " instrumental\n",
      " youre\n",
      " dramatic\n",
      " time\n",
      " cant\n",
      " shit\n",
      " continues\n",
      " could\n",
      "Cluster 2\n",
      " fucking\n",
      " dont\n",
      " fuck\n",
      " shit\n",
      " youre\n",
      " fuckin\n",
      " time\n",
      " mean\n",
      " really\n",
      " cant\n",
      "Cluster 3\n",
      " dont\n",
      " youre\n",
      " time\n",
      " cant\n",
      " really\n",
      " didnt\n",
      " mean\n",
      " could\n",
      " say\n",
      " something\n",
      "Cluster 4\n",
      " im\n",
      " ill\n",
      " dont\n",
      " ive\n",
      " rangers\n",
      " youre\n",
      " god\n",
      " ha\n",
      " thats\n",
      " mr\n",
      "Cluster 5\n",
      " lm\n",
      " lts\n",
      " lll\n",
      " lt\n",
      " lve\n",
      " lf\n",
      " dont\n",
      " ng\n",
      " ls\n",
      " ld\n",
      "Cluster 6\n",
      " dont\n",
      " youre\n",
      " didnt\n",
      " font\n",
      " time\n",
      " tell\n",
      " cant\n",
      " could\n",
      " way\n",
      " something\n",
      "Cluster 7\n",
      " font\n",
      " color\n",
      " dd\n",
      " ffff\n",
      " aaffi\n",
      " ff\n",
      " dont\n",
      " fontfont\n",
      " white\n",
      " ffi\n",
      "Cluster 8\n",
      " dont\n",
      " time\n",
      " youre\n",
      " cant\n",
      " way\n",
      " could\n",
      " say\n",
      " something\n",
      " make\n",
      " little\n",
      "Cluster 9\n",
      " ok\n",
      " dont\n",
      " youre\n",
      " cant\n",
      " time\n",
      " really\n",
      " something\n",
      " didnt\n",
      " could\n",
      " love\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = pipeline.named_steps['clust'].cluster_centers_.argsort()[:, ::-1]\n",
    "terms = pipeline.named_steps['vect'].get_feature_names()\n",
    "for i in range(10):\n",
    "    print(\"Cluster {}\".format(i)),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' {}'.format(terms[ind])),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Если не удалять имена, стабильно выделяется кластер Стартрека, также иногда кластеры ТБВ, Супернатуралов, Симпсонов\n",
    "\n",
    "Если удалять, получается странно, но звуки действительно кластеризуются"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## зачем это может быть нужно\n",
    "\n",
    "- не знаю, но это весело"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  Tensor(\"Placeholder:0\", shape=(1000, 60, 41, 2), dtype=float32)\n",
      "l1 filters  <tf.Variable 'Variable:0' shape=(57, 6, 2, 20) dtype=float32_ref>\n",
      "first layer  (1000, 60, 41, 20)\n",
      "here\n",
      "max pool  (1000, 60, 14, 20)\n",
      "filters  <tf.Variable 'Variable_2:0' shape=(1, 3, 20, 20) dtype=float32_ref>\n",
      "second layer  (1000, 60, 14, 20)\n",
      "max pool  (1000, 60, 5, 20)\n",
      "reshape (1000, 6000)\n",
      "<tf.Variable 'Variable_4:0' shape=(6000, 80) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_5:0' shape=(80,) dtype=float32_ref>\n",
      "third layer  (1000, 80)\n",
      "fourth shape [1000, 80]\n",
      "data  Tensor(\"Const:0\", shape=(8115, 60, 41, 2), dtype=float32)\n",
      "l1 filters  <tf.Variable 'Variable:0' shape=(57, 6, 2, 20) dtype=float32_ref>\n",
      "first layer  (8115, 60, 41, 20)\n",
      "max pool  (8115, 60, 14, 20)\n",
      "filters  <tf.Variable 'Variable_2:0' shape=(1, 3, 20, 20) dtype=float32_ref>\n",
      "second layer  (8115, 60, 14, 20)\n",
      "max pool  (8115, 60, 5, 20)\n",
      "reshape (8115, 6000)\n",
      "<tf.Variable 'Variable_4:0' shape=(6000, 80) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_5:0' shape=(80,) dtype=float32_ref>\n",
      "third layer  (8115, 80)\n",
      "fourth shape [8115, 80]\n",
      "data  Tensor(\"Const_1:0\", shape=(8260, 60, 41, 2), dtype=float32)\n",
      "l1 filters  <tf.Variable 'Variable:0' shape=(57, 6, 2, 20) dtype=float32_ref>\n",
      "first layer  (8260, 60, 41, 20)\n",
      "max pool  (8260, 60, 14, 20)\n",
      "filters  <tf.Variable 'Variable_2:0' shape=(1, 3, 20, 20) dtype=float32_ref>\n",
      "second layer  (8260, 60, 14, 20)\n",
      "max pool  (8260, 60, 5, 20)\n",
      "reshape (8260, 6000)\n",
      "<tf.Variable 'Variable_4:0' shape=(6000, 80) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_5:0' shape=(80,) dtype=float32_ref>\n",
      "third layer  (8260, 80)\n",
      "fourth shape [8260, 80]\n"
     ]
    }
   ],
   "source": [
    "bands = 60\n",
    "frames = 41\n",
    "num_labels = 10\n",
    "num_channels = 2\n",
    "\n",
    "batch_size = 1000\n",
    "patch1_h = 57\n",
    "patch1_w = 6\n",
    "patch2_h = 1\n",
    "patch2_w = 3\n",
    "depth = 20\n",
    "num_hidden = 80\n",
    "learning_rate = 0.002 # 0.05\n",
    "dropout = 0.5\n",
    "\n",
    "# nn.dropout() только на трейне\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # input\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, bands, frames, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset, dtype='float32_ref')\n",
    "    tf_test_dataset = tf.constant(test_dataset, dtype='float32_ref')\n",
    "  \n",
    "    # layers\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch1_h, patch1_w, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch2_h, patch2_w, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    \n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [frames // 8 * bands * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [frames // 10 * depth, num_hidden], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "    layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # model\n",
    "    def model(data, train=True):\n",
    "        # первый свёрточный\n",
    "        print('data ', data)\n",
    "        print('l1 filters ', layer1_weights)\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        print('first layer ', hidden.shape)\n",
    "        hidden = tf.nn.max_pool(hidden, ksize=[1, 4, 3, 1], strides=[1, 1, 3, 1],\n",
    "                          padding='SAME')\n",
    "        if train:\n",
    "            hidden = tf.nn.dropout(hidden, dropout)\n",
    "            print('here')\n",
    "        print('max pool ', hidden.shape)\n",
    "        # второй свёрточный\n",
    "        print('filters ', layer2_weights)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        print('second layer ', hidden.shape)\n",
    "        hidden = tf.nn.max_pool(hidden, ksize=[1, 1, 3, 1], strides=[1, 1, 3, 1],\n",
    "                          padding='SAME')\n",
    "        print('max pool ', hidden.shape)\n",
    "        # FC 1\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        print('reshape', reshape.shape)\n",
    "        print(layer3_weights)\n",
    "        print(layer3_biases)\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        print('third layer ', hidden.shape)\n",
    "        if train:\n",
    "            hidden = tf.nn.dropout(hidden, dropout)\n",
    "        # FC 2\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        print('fourth shape', shape)\n",
    "#         reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(hidden, layer4_weights) + layer4_biases)\n",
    "        if train:\n",
    "            hidden = tf.nn.dropout(hidden, dropout)\n",
    "        return tf.matmul(hidden, layer5_weights) + layer5_biases\n",
    "  \n",
    "    # training\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "    # predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset, train=False))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset, train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 270.738434\n",
      "Minibatch accuracy: 8.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 50/2000 [20:47<12:57:33, 23.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 50: 2.764158\n",
      "Minibatch accuracy: 9.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 60/2000 [25:13<15:00:31, 27.85s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-02f755becbce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         _, l, predictions = session.run(\n\u001b[0;32m---> 14\u001b[0;31m           [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minibatch loss at step %d: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Basilis/anaconda3/envs/diplom3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Basilis/anaconda3/envs/diplom3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Basilis/anaconda3/envs/diplom3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Basilis/anaconda3/envs/diplom3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Basilis/anaconda3/envs/diplom3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 2000\n",
    "\n",
    "saver = tf.train.Saver([layer1_weights])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in tqdm(range(num_steps)):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "#             valpred = session.run([valid_prediction], feed_dict={tf_valid_dataset: valid_dataset})\n",
    "#             print('Validation accuracy: %.1f%%' % accuracy(val_pred, valid_labels))\n",
    "            save_path = saver.save(session, \"model.ckpt\")\n",
    "#             print(\"Model saved in file: %s\" % save_path)\n",
    "#     print('Test accuracy: %.1f%%' % accuracy(session.run([test_prediction], feed_dict={tf_test_dataset: test_dataset}), test_labels))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "maga",
   "language": "python",
   "name": "maga"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
