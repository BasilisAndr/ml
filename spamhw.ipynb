{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) удаление стоп-слов, а также пороги минимальной и максимальной document frequency;\n",
    "\n",
    "4) векторизация документов (CountVectorizer vs. TfIdfVectorizer);\n",
    "\n",
    "5) что-нибудь ещё?\n",
    "\n",
    "При оценке классификатора обратите внимание на TP и FP.\n",
    "\n",
    "построить классификатор, оценить и записать результат; в итоге результаты усреднить. 2) поможет ли параметр class prior probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'smsspamcollection/SMSSpamCollection'\n",
    "\n",
    "messages = pd.read_csv(path, sep='\\t',\n",
    "                           names=[\"label\", \"message\"])\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузили, посмотрели. Смотрим на классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        message\n",
      "label                                                          \n",
      "ham   count                                                4825\n",
      "      unique                                               4516\n",
      "      top                                Sorry, I'll call later\n",
      "      freq                                                   30\n",
      "spam  count                                                 747\n",
      "      unique                                                653\n",
      "      top     Please call our customer service representativ...\n",
      "      freq                                                    4\n"
     ]
    }
   ],
   "source": [
    "print(messages.groupby('label').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выборка очевидно несбалансирована по классам (~6:1). Соответственно, dummy-classifier, всегда присваивающий ярлык \"ham\", будет иметь accuracy ~86%, а confusion matrix будет выглядеть вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf_ham</th>\n",
       "      <th>clf_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      clf_ham  clf_spam\n",
       "ham      4825         0\n",
       "spam      747         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'clf_ham': pd.Series([4825, 747], index=['ham', 'spam']),\n",
    "    'clf_spam': pd.Series([0, 0], index=['ham', 'spam'])}\n",
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для задачи определения спама такой классификатор абсолютно бесполезен просто по определению: это классификатор, который никогда не определяет спам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соотношение 6:1 - вот и отлично, разделим ham на шесть непересекающихся выборок, обучим классификатор с каждым"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# токенизируем, лемматизируем, стеммим, всё вот это запихиваем в один датафрейм\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "msg_tok = messages['message'].map(tokenize)\n",
    "\n",
    "\n",
    "def get_rid(arr):\n",
    "    res = [x.strip(string.punctuation) for x in arr if x.strip(string.punctuation) != '']\n",
    "    return res\n",
    "\n",
    "\n",
    "msg_tok_no_punct = msg_tok.map(get_rid)\n",
    "\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "st = LancasterStemmer()\n",
    "\n",
    "\n",
    "def lemmatize(arr):\n",
    "    return list(map(lm.lemmatize, arr))\n",
    "\n",
    "\n",
    "def stem(arr):\n",
    "    return list(map(st.stem, arr))\n",
    "\n",
    "msg_df = pd.DataFrame({'tokenized_punct': msg_tok, 'tokenized_no_punct': msg_tok_no_punct, \n",
    "                    'punctlem': msg_tok.map(lemmatize), 'punctstem': msg_tok.map(stem), \n",
    "                    'nopunctlem': msg_tok_no_punct.map(lemmatize), 'nopunctstem': msg_tok_no_punct.map(stem), \n",
    "                    'label': messages['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>nopunctlem</th>\n",
       "      <th>nopunctstem</th>\n",
       "      <th>punctlem</th>\n",
       "      <th>punctstem</th>\n",
       "      <th>tokenized_no_punct</th>\n",
       "      <th>tokenized_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>[go, until, jurong, point, crazy, avail, on, i...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy.., avail, ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, jok, wif, u, on]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., jok, wif, u, on, ...]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[fre, entry, in, 2, a, wkly, comp, to, win, fa...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[fre, entry, in, 2, a, wkly, comp, to, win, fa...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, so, ear, hor, u, c, already, the...</td>\n",
       "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
       "      <td>[u, dun, say, so, ear, hor, ..., u, c, already...</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[nah, i, do, n't, think, he, go, to, usf, he, ...</td>\n",
       "      <td>[nah, i, do, n't, think, he, goe, to, usf, he,...</td>\n",
       "      <td>[nah, i, do, n't, think, he, go, to, usf, ,, h...</td>\n",
       "      <td>[nah, i, do, n't, think, he, goe, to, usf, ,, ...</td>\n",
       "      <td>[nah, i, do, n't, think, he, goes, to, usf, he...</td>\n",
       "      <td>[nah, i, do, n't, think, he, goes, to, usf, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                         nopunctlem  \\\n",
       "0   ham  [go, until, jurong, point, crazy, available, o...   \n",
       "1   ham                     [ok, lar, joking, wif, u, oni]   \n",
       "2  spam  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3   ham  [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4   ham  [nah, i, do, n't, think, he, go, to, usf, he, ...   \n",
       "\n",
       "                                         nopunctstem  \\\n",
       "0  [go, until, jurong, point, crazy, avail, on, i...   \n",
       "1                         [ok, lar, jok, wif, u, on]   \n",
       "2  [fre, entry, in, 2, a, wkly, comp, to, win, fa...   \n",
       "3  [u, dun, say, so, ear, hor, u, c, already, the...   \n",
       "4  [nah, i, do, n't, think, he, goe, to, usf, he,...   \n",
       "\n",
       "                                            punctlem  \\\n",
       "0  [go, until, jurong, point, ,, crazy.., availab...   \n",
       "1           [ok, lar, ..., joking, wif, u, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [u, dun, say, so, early, hor, ..., u, c, alrea...   \n",
       "4  [nah, i, do, n't, think, he, go, to, usf, ,, h...   \n",
       "\n",
       "                                           punctstem  \\\n",
       "0  [go, until, jurong, point, ,, crazy.., avail, ...   \n",
       "1               [ok, lar, ..., jok, wif, u, on, ...]   \n",
       "2  [fre, entry, in, 2, a, wkly, comp, to, win, fa...   \n",
       "3  [u, dun, say, so, ear, hor, ..., u, c, already...   \n",
       "4  [nah, i, do, n't, think, he, goe, to, usf, ,, ...   \n",
       "\n",
       "                                  tokenized_no_punct  \\\n",
       "0  [go, until, jurong, point, crazy, available, o...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4  [nah, i, do, n't, think, he, goes, to, usf, he...   \n",
       "\n",
       "                                     tokenized_punct  \n",
       "0  [go, until, jurong, point, ,, crazy.., availab...  \n",
       "1           [ok, lar, ..., joking, wif, u, oni, ...]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
       "3  [u, dun, say, so, early, hor, ..., u, c, alrea...  \n",
       "4  [nah, i, do, n't, think, he, goes, to, usf, ,,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но в таком виде ни один векторайзер ничего не примет (как оказалось), поэтому попробуем сджойнить листы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>nopunctlem</th>\n",
       "      <th>nopunctstem</th>\n",
       "      <th>punctlem</th>\n",
       "      <th>punctstem</th>\n",
       "      <th>tokenized_no_punct</th>\n",
       "      <th>tokenized_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go until jurong point crazy avail on in bug n ...</td>\n",
       "      <td>go until jurong point , crazy.. available only...</td>\n",
       "      <td>go until jurong point , crazy.. avail on in bu...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go until jurong point , crazy.. available only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar jok wif u on</td>\n",
       "      <td>ok lar ... joking wif u oni ...</td>\n",
       "      <td>ok lar ... jok wif u on ...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar ... joking wif u oni ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                         nopunctlem  \\\n",
       "0   ham  go until jurong point crazy available only in ...   \n",
       "1   ham                            ok lar joking wif u oni   \n",
       "\n",
       "                                         nopunctstem  \\\n",
       "0  go until jurong point crazy avail on in bug n ...   \n",
       "1                                ok lar jok wif u on   \n",
       "\n",
       "                                            punctlem  \\\n",
       "0  go until jurong point , crazy.. available only...   \n",
       "1                    ok lar ... joking wif u oni ...   \n",
       "\n",
       "                                           punctstem  \\\n",
       "0  go until jurong point , crazy.. avail on in bu...   \n",
       "1                        ok lar ... jok wif u on ...   \n",
       "\n",
       "                                  tokenized_no_punct  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "\n",
       "                                     tokenized_punct  \n",
       "0  go until jurong point , crazy.. available only...  \n",
       "1                    ok lar ... joking wif u oni ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in msg_df.columns[1:]:\n",
    "    msg_df[col] = msg_df[col].map(lambda x: ' '.join(x))\n",
    "msg_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Окей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# строим пайплайны\n",
    "CV_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()), \n",
    "                   ('clf', MultinomialNB()),])\n",
    "TI_clf = Pipeline([('vect', TfidfVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()), \n",
    "                   ('clf', MultinomialNB()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# и настраиваем gridsearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__stop_words': [None, 'english'],\n",
    "              'vect__min_df': [1, 3],\n",
    "              'vect__max_df': [1.0, 0.8],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# теперь будем это всё делить.\n",
    "# и запускать.\n",
    "\n",
    "ham = msg_df.loc[msg_df['label']=='ham']\n",
    "spam = msg_df.loc[msg_df['label']=='spam']\n",
    "\n",
    "\n",
    "gs_stats = {}\n",
    "len_ham_sample = len(ham)//6\n",
    "\n",
    "# 0 колонка - это label, его не перебираем\n",
    "for column in msg_df.columns[1:]:\n",
    "    for i in range(6):\n",
    "        # делим на шесть равных выборок (игнорируем в итоге не больше пяти наблюдений)\n",
    "        hham = ham[i*len_ham_sample:i*len_ham_sample+len_ham_sample]\n",
    "        # очень плохо, что они не перемешаны, ноо\n",
    "        sample = pd.concat([spam, hham])\n",
    "        X = sample[column]\n",
    "        y = sample['label']\n",
    "        gs_cv = GridSearchCV(CV_clf, parameters)\n",
    "        gs_ti = GridSearchCV(TI_clf, parameters)\n",
    "        gs_cv = gs_cv.fit(X, y)\n",
    "        gs_ti = gs_ti.fit(X, y)\n",
    "        gs_stats[column] = [gs_cv]\n",
    "        gs_stats[column].append(gs_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'clf__alpha': 1.0,\n",
       " 'clf__class_prior': None,\n",
       " 'clf__fit_prior': True,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__vocabulary': None}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_stats['punctlem'][0].best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4198"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gs_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham from 0 to 804\n",
      "fitting CountVectorizer...\n",
      "fitting TfidfVectorizer...\n",
      "ham from 804 to 1608\n",
      "fitting CountVectorizer...\n",
      "fitting TfidfVectorizer...\n",
      "ham from 1608 to 2412\n",
      "fitting CountVectorizer...\n",
      "fitting TfidfVectorizer...\n",
      "ham from 2412 to 3216\n",
      "fitting CountVectorizer...\n",
      "fitting TfidfVectorizer...\n",
      "ham from 3216 to 4020\n",
      "fitting CountVectorizer...\n",
      "fitting TfidfVectorizer...\n",
      "ham from 4020 to 4824\n",
      "fitting CountVectorizer...\n",
      "fitting TfidfVectorizer...\n"
     ]
    }
   ],
   "source": [
    "column = 'nopunctlem'\n",
    "stats = []\n",
    "len_ham_sample = len(ham)//6\n",
    "for i in range(6):\n",
    "    # делим на шесть равных выборок (игнорируем в итоге не больше пяти наблюдений)\n",
    "    hham = ham[i*len_ham_sample:i*len_ham_sample+len_ham_sample]\n",
    "    print('ham from {} to {}'.format(i*len_ham_sample, i*len_ham_sample+len_ham_sample))\n",
    "    # очень плохо, что они не перемешаны, ноо\n",
    "    sample = pd.concat([spam, hham])\n",
    "    X = sample[column]\n",
    "    y = sample['label']\n",
    "    # строим пайплайны\n",
    "    CV_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()), \n",
    "                   ('clf', MultinomialNB()),])\n",
    "    TI_clf = Pipeline([('vect', TfidfVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()), \n",
    "                   ('clf', MultinomialNB()),])\n",
    "    gs_cv = GridSearchCV(CV_clf, parameters)\n",
    "    gs_ti = GridSearchCV(TI_clf, parameters)\n",
    "    print('fitting CountVectorizer...')\n",
    "    gs_cv = gs_cv.fit(X, y)\n",
    "    print('fitting TfidfVectorizer...')\n",
    "    gs_ti = gs_ti.fit(X, y)\n",
    "    stats.append(gs_cv)\n",
    "    stats.append(gs_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'clf__alpha': 1.0,\n",
       " 'clf__class_prior': None,\n",
       " 'clf__fit_prior': True,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__vocabulary': None}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats[6].best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2(+2). Сравнить результаты байесовского классификатора, решающего дерева и RandomForest. Помимо стандартных метрик оценки качества модели, необходимо построить learning curve, ROC-curve, classification report и интерпретировать эти результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3(+2). А что, если в качестве предикторов брать не количество вхождений слов, а конструировать специальные признаки? Прежде всего, необходимо разделить таблицу на training set и test set в соотношении 80:20, test set не открывать до этапа оценки модели. С помощью pandas проверить, отличаются ли перечисленные ниже параметры (иможно придумать другие) для разных классов (spam/ham), и собрать матрицу признаков для обучения. Примеры признаков: длина сообщения, количество букв в ВЕРХНЕМ РЕГИСТРЕ, восклицательных знаков, цифр, запятых, каких-то конкретных слов (для этого можно построить частотный словарь по сообщениям каждого класса). Прокомментировать свой выбор. Векторизовать документы и построить классификатор. Оценить модель на проверочной выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages['length'] = messages['message'].map(lambda text: len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(messages.head())\n",
    "\n",
    "# tokens = [word_tokenize(msg) for msg in messages]\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# messages.message.head().apply(tokenize)\n",
    "# messages.message = messages.message.apply(tokenize)\n",
    "\n",
    "# print(messages.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow = CountVectorizer()\n",
    "bow.fit_transform(messages['message'])\n",
    "# print(bow.vocabulary_)\n",
    "\n",
    "# m = messages['message'][3]\n",
    "# print([m])\n",
    "# bowed_m = bow.transform([m])\n",
    "# print(bowed_m.shape)\n",
    "# print(bow.get_feature_names()[1054])\n",
    "\n",
    "bowed_messages = bow.transform(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# naive_model.predict()\n",
    "\n",
    "# msg_train, msg_test, label_train, label_test = train_test_split(messages['message'], messages['label'], test_size=0.2)\n",
    "# print(len(msg_train), len(msg_test))\n",
    "cv_results = cross_val_score(naive_model, bowed_messages, messages['label'], cv=10, scoring='accuracy')\n",
    "print(cv_results.mean(), cv_results.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('bow', CountVectorizer(analyzer=tokenize)),\n",
    "#     ('classifier', MultinomialNB()),\n",
    "# ])\n",
    "#\n",
    "# cv_results = cross_val_score(pipeline,\n",
    "#                              msg_train,\n",
    "#                              label_train,\n",
    "#                              cv=10,\n",
    "#                              scoring='accuracy',\n",
    "#                              )\n",
    "# print(cv_results.mean(), cv_results.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
